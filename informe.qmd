---
title: "Inferencia Conformal"
author: "Mauro Loprete y Maximiliano Saldaña"
format: 
  pdf:
    documentclass: scrreprt
date: "Diciembre 2022"
number-sections: true
toc: true
bibliography: biblio.bib
csl: apa-with-abstract.csl
lang: es
execute:
  echo: false
  include: false
  warning: false
crossref:
  chapters: true
header-includes:
  - \usepackage[linesnumbered,ruled,vlined]{algorithm2e}
  - \usepackage{xcolor}
  - \SetKwInput{KwInput}{Entrada}                
  - \SetKwInput{KwOutput}{Salida}
  - \SetAlgorithmName{Algoritmo}
---

# Introducción {#sec-intro}

En @lei2017 los autores plantean un marco general para realizar inferencia predictiva sin supuestos distribucionales en un contexto de regresión, empleando la _inferencia conformal_. Mediante la metodología planteada se pueden obtener intervalos de confianza con validez en muestra finitas (no asintótica) para una variable de respuesta, empleando cualquier estimador de la función de regresión.

El problema se plantea de la siguiente manera: Se considera $Z_1, \dots, Z_n \sim F$ i.i.d., donde $Z_i = (X_i, Y_i)$ es una variable aleatoria en $\mathbb{R}^d \times \mathbb{R}$, $Y_i$ es la variable de respuesta y $X_i = X_i(1) \dots, X_i(d)$ son las covariables. Se tiene tiene la función de regresión:

$$
\mu(x) = E(Y | X = x), \,\,\,\, x \in \mathbb{R}^d
$$

Es de interés predecir la nueva respuesta $Y_{n+1}$ a las covariables $X_{n+1}$, sin hacer supuestos sobre $\mu$ o $F$. Dado un nivel de cobertura $\alpha$, el objetivo es construir un intervalo de predicción $C \subseteq \mathbb{R}^d \times \mathbb{R}$ basado en $Z_1, \dots, Z_n$ que cumpla:

$$
P(Y_{n+1} \in C(X_{n+1})) \ge 1-\alpha
$$

En esta expresión se supone que $Z_{n+1} = (X_{n+1}, Y_{n+1})$ proviene también de la distribución $F$ y $C(x) = \{y \in \mathbb{R}: (x,y) \in C\}, \,\,\, x \in \mathbb{R}^d$

# Inferencia conformal {#sec-InfConf}

La idea básica de la inferencia conformal, dadas las definiciones de la introducción, consiste en que para decidir si un valor $y$ está incluido en el intervalo $C(X_{n+1})$ consideramos poner a prueba la hipótesis nula de que $Y_{n+1} = y$ y se construye un valor-p válido basado en los cuantiles empíricos de la muestra aumentada $Z_1, \dots, Z_n, Z_{n+1}$.

## Un resultado previo {#sec-resprevio}

Sean $U_1, \dots, U_n$ una muestra i.i.d de una variable aleatoria continua. Para un nivel de no cobertura $\alpha \in (0,1)$ y una observación $U_{n+1}$, nótese que:

$$
P(U_{n+1} \le \hat{q}_{1-\alpha}) \ge 1-\alpha
$${#eq-Uresprevio}

Donde $\hat{q}_{1-\alpha}$ es el cuantil de la muestra $U_1, \dots, U_n$ definido por:

$$
\hat{q}_{1-\alpha} = \begin{cases}
  U_{(\lceil (n+1)(1-\alpha) \rceil)} \,\,\,\, \text{si} \,\,\,\, \lceil (n+1)(1-\alpha) \rceil \le n \\
  \infty \,\,\,\, \text{en caso contrario}
\end{cases}
$${#eq-Ucases}

Aquí $U_{(1)} \le \dots \le U_{(n)}$ son los estadísticos de orden de la muestra. Se verifica la cobertura en muestra finita de la @eq-Uresprevio: dada la independencia de las variables, el rango de $U_{n+1}$ en la muestra se distribuye uniforme en el conjunto $\{1, \dots, n+1\}$, entonces 

## Método _naive_ de construcción de intervalos {#sec-naive}

Usando el resultado previo de la sección anterior y en el contexto de regresión planteado en la Sección @sec-intro, un método sencillo para contruir un intervalo predictivo para $Y_{n+1}$ ante el valor $X_{n+1}$ es:

$$
C_{naive}(X_{n+1}) = \left[ \hat{\mu}(X_{n+1}) - \hat{F}^{-1}_n (1-\alpha), \hat{\mu}(X_{n+1}) + \hat{F}^{-1}_n (1-\alpha) \right]
$${#eq-Cnaive}

donde $\hat{\mu}$ es un estimador de la función de regresión, $\hat{F}_n$ la distribución empírica de los residuos dentro de la muestra $|Y_i - \hat{\mu}(X_{i})|$, $i=1,\dots,n$ y ${F}^{-1}_n (1-\alpha)$ el cuantil $1-\alpha$ de $\hat{F}_n$.

Este método es aproximadamente válido para muestras grandes, bajo la condición de que $\hat{\mu}$ sea lo suficientemente preciso, es decir, que ${F}^{-1}_n (1-\alpha)$ esté cerca del cuantil $1-\alpha$ de $|Y_i - \mu(X_{i})|$. Para que esto se cumpla en general es necesario el cumplimiento de condiciones de regularidad tanto para la distribución $F$ de los datos y para $\hat{mu}$, como que el modelo esté correctamente especificado.   

Un problema de este método es que los intervalos pueden presentar una considerable sub-cobertura, dado que se están empleando los residuos dentro de la muestra. Para subsanar esto, en @lei2017 se plantea la metodología de los intervalos de predicción conformales.

## Intervalos de predicción conformales

Para cada valor $y \in \mathbb{R}$ se construye un estimador de regresión aumentado $\hat{\mu}_y$, el cual se estima en el conjunto de datos aumentado $Z_1,\dots\,Z_n, (X_{n+1}, y)$. Luego, se define:

$$R_{y,i} = |Y_i - \hat{\mu}_y(X_i)|, \,\,\, i = 1,\dots,n$${#eq-residuoconf1}

$$R_{y,n+1} = |y - \hat{\mu}_y(X_{n+1})|$${#eq-residuoconf2}

Con el rango de $R_{y,n+1}$ entre los demás residuos de la muestra $R_{y,1}, \dots, R_{y,n}$ se calcula:

$$
\pi(y) = \frac{1}{n+1}\sum_{i=1}^{n+1}\mathbb{I}\{R_{y,i} \le R_{y,n+1}\} = \frac{1}{n+1} + \frac{1}{n+1}\sum_{i=1}^{n}\mathbb{I}\{R_{y,i} \le R_{y,n+1}\}
$${#eq-piy}

que es la proporción de los puntos de la muestra aumentada cuyos residuos dentro de la muestra son más pequeños que el residuo $R_{y,n+1}$. Como los datos son i.i.d. y suponiendo la simetría de $\hat{\mu}$, se puede apreciar que el estadístico $\pi(Y_{n+1})$ se distribuye uniforme en ${1/(n+1), 2/(n+1),\dots,1}$, lo cual implica^[Ver por qué]:

$$
P((n+1)\pi(Y_{n+1})\le \lceil (1-\alpha)(n+1)\rceil) \ge 1-\alpha
$${#eq-probconf}

Esta expresión se puede interpretar como que $1-\pi(Y_{n+1})$ da un valor-p válido conservador para la prueba de hipótesis donde $H_0) Y_{n+1} =y$.

Aplicando dicha prueba sobre todos los posibles valores de $y \in \mathbb{R}$, la ecuación @eq-probconf lleva al intervalo de predicción conformal evaluado en $X_{n+1}$:

$$
C_{conf}(X_{n+1}) = \left[y \in \mathbb{R}: (n+1)\pi(Y_{n+1})\le \lceil (1-\alpha)(n+1) \rceil \right]
$${#eq-Cconf}

Cada vez que se quiere obtener un intervalo de predicción en un nuevo conjunto de covariables se tienen que recalcular los pasos @eq-residuoconf1, @eq-residuoconf2, @eq-piy y @eq-Cconf. En la práctica, se restringen los valores de $y$ a una grilla discreta.

El procedimiento para obtener el intervalo se puede resumir en el Algoritmo \ref{alg:algo1}.


\begin{algorithm}[!ht]
\caption{Intervalo de predicción conformal \label{alg:algo1}}

\DontPrintSemicolon
  
  \KwInput{Datos $(X_i, Y_i)$, $i =1, \dots, n$, nivel de no cobertura $\alpha \in (0,1)$, algoritmo de regresión $\mathcal{A}$, puntos $\mathcal{X}_{nuevo}$ en los que construir intervalos de predicción y valores $\mathcal{Y}_{prueba} = \{y_1,y_2,\dots\}$ para comparar con la predicción.}


  \KwOutput{Intervalos de predicción, en cada elemento de $\mathcal{X}_{nuevo}$}
  
  \For{$x \in \mathcal{X}_{nuevo}$}
    {
      \For{$y \in \mathcal{Y}_{prueba}$}
        {
          $\hat{\mu}_y = \mathcal{A}(\{(X_1, Y_1), \dots, (X_n,Y_n), (x,y)\})$\;

          $R_{y_i} = |Y_i-\hat{\mu}_y(X_i)|, \,\, i=1,\dots,n$ y $R_{y,n+1}=|y-\hat{\mu}_y(x)|$\;

          $\pi(y) = (1+\sum_{i=1}^n \mathbb{I}\{R_{y,i} \le R_{y, n+1} \})/(n+1)$\;
        } 

        $C_{conf}(x) = \left[y \in \mathbb{R}: (n+1)\pi(Y_{n+1})\le \lceil (1-\alpha)(n+1) \rceil  \right]$\;
    }
    Se devuelve $C_{conf}(x)$ para cada $X \in \mathcal{X}_{nuevo}$.

\end{algorithm}


### Teorema {#sec-teorema1}

El intervalo @eq-Cconf tiene cobertura válida para muestras finitas por construcción y a su vez no presenta sobrecobertura. Esto se puede expresar mediante las expresiones @eq-cobCconf y @eq-cobCconf2, respectivamente:

Sea $(X_i, Y_i)$, $i=1,\dots,n$ v.a. i.i.d, entonces para la nueva observación i.i.d. $(X_{n+1}, Y_{n+1})$:

$$
P(Y_{n+1}\in C_{conf}(X_{n+1})) \ge 1-\alpha
$${#eq-cobCconf}

Adicionalmente, si se hace el supuesto que para todo $y \in \mathbb{R}$ los residuos dentro de la muestra $R_{y,i} = |Y_i - \hat{\mu}_y(X_i)|$, $i=1,\dots,n$ tienen una distribución conjunta continua se cumple que:

$$
P(Y_{n+1}\in C_{conf}(X_{n+1})) \le 1-\alpha + \frac{1}{n+1}
$${#eq-cobCconf2}


::: {.remark}
Nótese que las probabilidades aquí, al tomarse sobre la muestra aumentada i.i.d. implican cobertura promedio (o marginal). Esto no es lo mismo que la cobertura condicional $P(Y_{n+1} \in C_{conf}(x)| X_{n+1}=x) \ge 1-\alpha \,\,\, \forall \,\,\, x \in \mathbb{R}^d$. Esta última es una propiedad más fuerte y no puede lograrse con intervalos predictivos de amplitud finita sin que el modelo y el estimador cumplan condiciones de regularidad y consistencia.
:::


::: {.remark}
Si se mejora el estimador $\hat{\mu}$, en general el intervalo de predicción conformal decrece en tamaño. Esto se da debido a que un $\hat{\mu}$ más preciso lleva a residuos más pequeños y los intervalos conformales están definidos por los cuantiles de la distribución aumentada de los residuos.
:::

## Intervalos de predicción conformales con muestras separadas

Un problema práctico de los intervalos de inferencia conformal de la sección anterior es que tienen mucho costo computacional. Para poder concluir si $y \in C_{conf}(X_{n+1})$, para cualquier $X_{n+1}$ y $y$, se tiene que reestimar el modelo en la muestra aumentada que incluye el nuevo punto $X_{n+1}$ y recalcular y reordenar los nuevos residuos obtenidos.

Para enfrentar esta problemática se puede hacer uso de una metodología denominada por @lei2017 como predicción conformal separada (_split conformal prediction_). Su costo computacional es menor (es el del paso de estimación únicamente) y tiene menos requerimientos de memoria (solo hay que guardar las variables seleccionadas cuando se evalúa el ajuste en los nuevos puntos $X_i$, $i \in \mathcal{I}_2$). Se presenta en el Algoritmo \ref{alg:algo2}.  

\begin{algorithm}[!ht]
\caption{Intervalos de predicción conformales con muestras separadas \label{alg:algo2}}

\DontPrintSemicolon
  
  \KwInput{Datos $(X_i, Y_i)$, $i =1, \dots, n$, nivel de no cobertura $\alpha \in (0,1)$, algoritmo de regresión $\mathcal{A}$.}

  \KwOutput{Intervalos de predicción, sobre $x \in \mathbb{R}^d$}
  
    Se separa la muestra al azar en dos subconjuntos de igual tamaño $\mathcal{I}_1$ e $\mathcal{I}_2$. \;

    $\hat{\mu}_y = \mathcal{A}(\{(X_i, Y_i): i \in \mathcal{I}_1 \})$\;

    $R_{i} = |Y_i-\hat{\mu}_y(X_i)|$, $i \in \mathcal{I}_2$\;

    $d=$ el k-ésimo valor más pequeño en $\{R_i: i \in \mathcal{I}_2\}$, donde $k=\lceil (n/2 +1)(1-\alpha) \rceil$ \;
    
    Se devuelve $C_{split}(x)=[\hat{\mu}-d, \hat{\mu}+d]$ para todo $x \in \mathbb{R}^d$.

\end{algorithm}

### Teorema 

Sea $(X_i, Y_i)$, $i=1,\dots,n$ v.a. i.i.d, entonces para la nueva observación i.i.d. $(X_{n+1}, Y_{n+1})$:

$$
P(Y_{n+1}\in C_{splt}(X_{n+1})) \ge 1-\alpha
$${#eq-cobCsplit}

Adicionalmente, si se hace el supuesto que los residuos $R_{i}$, $i \in \mathcal{I}_2$ tienen una distribución conjunta continua se cumple que:

$$
P(Y_{n+1}\in C_{split}(X_{n+1})) \le 1-\alpha + \frac{2}{n+2}
$${#eq-cobCsplit2}

### Teorema

Los intervalos de predicción conformales con muestras separadas dan una garantía aproximada de cobertura dentro de la muestra. Esto se puede expresar como que existe una constante $c>0$ tal que, para cualquier $\epsilon>0$:

$$
P\left( \frac{2}{n} \sum_{i \in \mathcal{I}_2} \mathbb{I} \{Y_I \in C_{split}(X_i) - (1-\alpha) \ge \epsilon\} \right) \le 2 \exp{(-cn^2(\epsilon-4/n)^2)}
$${#eq-cobCsplitInSample}

Esto implica cobertura dentro de la muestra para la muestra $\mathcal{I}_2$, revirtiendo los roles de $\mathcal{I}_1$ e $\mathcal{I}_2$ se puede extender para toda la muestra.

::: {.remark}
También se puede aplicar este método con una separación no balanceada de la muestra, con $|\mathcal{I}_1|= \rho n$ e $|\mathcal{I}_1|= \rho n$, para $\rho \in (0,1)$. Esto puede ser útil en situaciones donde el procedimiento de regresión es complejo y puede resultar beneficioso elegir $\rho > 0,5$, para que $\hat{\mu}$ sea más preciso.
::: 

## Intervalos conformales con múltiples separaciones de la muestra

Si bien separar la muestra reduce el tiempo que se tarda en calcular los intervalos, introduce otro elemento aleatorio en el método que incrementa la variabildiad; que es cuáles observaciones quedan en uno u otro de los subconjuntos. Una manera de enfrentarse a esto es combinar las inferencias realizadas con distintas separaciones de la muestra. 

Sea $N$ la cantidad de veces que separamos la muestra, se calculan con estos distintos subconjuntos los intervalos de predicción conformales $C_{split,1},\dots,C_{split,N}$, donde cada intervalo se construye al nivel de confianza $1-\alpha/N$. Se define:

$$
C_{split}^{(N)}(x) = \bigcap_{j=1}^N C_{split,j}(x), \,\,\, \text{sobre} \,\,\, x \in \mathbb{R}^d
$$ {#eq-CMultSplit}

Usando un argumento del tipo Bonferroni^[Ver esto], se concluye que la banda de predicción $C_{split}^{(N)}$ tiene una cobertura marginal de por lo menos $1-\alpha$.

Esta metodología reduce la variabilidad originada por la separación, pero se puede dar que el tamaño del intervalo $C_{split}^{(N)}$ es creciente en $N$ y es más amplio que $C_{split}$, debido al nivel de confianza con que se construye cada intervalo. 

## Intervalos predictivos mediante Jackknife

Esta metodología emplea los cuantiles de los residuos de validación cruzada dejando una observación fuera (_leave-one-out_) para calcular los intervalos de predicción. 


\begin{algorithm}[!ht]
\caption{Intervalo de predicción conformal mediante Jackknife. \label{alg:algo3}}

\DontPrintSemicolon
  
  \KwInput{Datos $(X_i, Y_i)$, $i =1, \dots, n$, nivel de no cobertura $\alpha \in (0,1)$, algoritmo de regresión $\mathcal{A}$.}


  \KwOutput{Intervalos de predicción sobre $x \in \mathbb{R}^d$.}
  
  \For{$i \in \{1, \dots, n \}$}
    {
      $\hat{\mu}^{(-i)} = \mathcal{A}(\{(X_l, Y_l): l \ne i\})$\;

      $R_{i} = |Y_i - \hat{\mu}^{(-i)}(X_i)|$\;
    }
    
    $d =$ el k-ésimo valor más pequeño en $\{R_i: i \in \{1, \dots, n\} \}$, con $k = \lceil n(1-\alpha)\rceil$\;
    
    Se devuelve $C_{jack}(x) = [\hat{\mu}(x)-d,\hat{\mu}(x)+d]$ para todo $x \in \mathbb{R}^d$\;

\end{algorithm}








Tiene la ventaja que emplea más de la muestra que se aparta para entrenar cuando se calculan los residuos, lo cual frecuentemente lleva a intervalos de menor amplitud. Como desventaja, los intervalos que se obtienen no garantizan cobertura válida fuera de la muestra cuando se trabaja con muestras finitas e incluso asintóticamente la cobertura depende de condiciones del estimador.





# Aplicación

```{r}
library(datasets)

cars
```


# Conclusión

# Bibliografía

::: {#refs}
:::

# Anexo {.appendix}


